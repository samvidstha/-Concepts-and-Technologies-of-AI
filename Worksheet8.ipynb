{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8YNQCjjKFQbF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDQLUVa2F3b2"
      },
      "source": [
        "1. Custom Decision Tree (from scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PiPYYZCPFgbq"
      },
      "outputs": [],
      "source": [
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(unique_classes) == 1:\n",
        "            return {'class': unique_classes[0]}\n",
        "        if num_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        # Find best split\n",
        "        best_info_gain = -float('inf')\n",
        "        best_split = None\n",
        "        for feature_idx in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_idx] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "                left_y, right_y = y[left_mask], y[right_mask]\n",
        "                info_gain = self._information_gain(y, left_y, right_y)\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask\n",
        "                    }\n",
        "\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth+1)\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_tree': left_tree,\n",
        "            'right_tree': right_tree\n",
        "        }\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        parent_entropy = self._entropy(parent)\n",
        "        left_entropy = self._entropy(left)\n",
        "        right_entropy = self._entropy(right)\n",
        "        weighted_avg = (len(left)/len(parent))*left_entropy + (len(right)/len(parent))*right_entropy\n",
        "        return parent_entropy - weighted_avg\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        probs = np.bincount(y) / len(y)\n",
        "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_single(x, self.tree) for x in X]\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "        feature_val = x[tree['feature_idx']]\n",
        "        if feature_val <= tree['threshold']:\n",
        "            return self._predict_single(x, tree['left_tree'])\n",
        "        else:\n",
        "            return self._predict_single(x, tree['right_tree'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP1ygItxF_Pm"
      },
      "source": [
        "2. Iris dataset: Custom vs Scikit-learn Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kki01k9iFmVE",
        "outputId": "28a37ba9-534d-4317-aaf5-365f5ba8e957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Decision Tree Comparison (Iris)\n",
            "Custom Decision Tree Accuracy: 1.0000\n",
            "Scikit-learn Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom tree\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Scikit-learn tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(\"\\n Decision Tree Comparison (Iris)\")\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "428emOu4GCSu"
      },
      "source": [
        "3. Ensemble Methods (Wine dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw5KJjISFrXt",
        "outputId": "d3211b28-aeb8-4173-df49-d9bed8776a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Ensemble Methods (Wine Classification) \n",
            "Decision Tree F1 Score: 0.9425\n",
            "Random Forest F1 Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average='macro')\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "print(\"\\n Ensemble Methods (Wine Classification) \")\n",
        "print(f\"Decision Tree F1 Score: {f1_dt:.4f}\")\n",
        "print(f\"Random Forest F1 Score: {f1_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zADDS62jGGN0"
      },
      "source": [
        "4. Hyperparameter Tuning (Random Forest Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zs8qUhgFu9_",
        "outputId": "24ab954b-488f-4dc3-9159-00d9394cc517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Hyperparameter Tuning (Random Forest Classifier) \n",
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best F1 Score: 0.9862945382658644\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                           param_grid, cv=3, scoring='f1_macro')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n Hyperparameter Tuning (Random Forest Classifier) \")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1 Score:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J45Wu9yGJBO"
      },
      "source": [
        "5. Regression Models (Wine dataset features â†’ target continuous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kef8W8i_Fx4B",
        "outputId": "1360ffd4-90d9-454d-e212-7e2484a5442b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Regression Models (Wine) \n",
            "Decision Tree MSE: 0.1667\n",
            "Random Forest MSE: 0.0648\n",
            "\n",
            " Hyperparameter Tuning (Random Forest Regressor) \n",
            "Best Parameters: {'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10}\n",
            "Best Score (Negative MSE): -0.05090262509850276\n"
          ]
        }
      ],
      "source": [
        "# For demonstration, use wine data but treat target as continuous regression\n",
        "y_wine_reg = y_wine.astype(float)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\n Regression Models (Wine) \")\n",
        "print(f\"Decision Tree MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest MSE: {mse_rf:.4f}\")\n",
        "\n",
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=5, cv=3, scoring='neg_mean_squared_error',\n",
        "                                   random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n Hyperparameter Tuning (Random Forest Regressor) \")\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Score (Negative MSE):\", random_search.best_score_)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}